{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpmZERK3Hf2n"
      },
      "source": [
        "얼리엑세스검색후\n",
        "\n",
        "포스트 제목 + 코멘트 가져오기\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "rYBH-ut54Fq6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_in_reddit(total_urls):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    results = []\n",
        "    for url in total_urls:\n",
        "        page = get(url, headers=headers)\n",
        "        if page.status_code != 200:\n",
        "            print(f\"Error: Could not retrieve {url}\")\n",
        "            continue\n",
        "        soup = BeautifulSoup(page.text, 'html.parser')\n",
        "        titles = soup.find_all(\"header\", class_=\"search-result-header\")\n",
        "        for title_section in titles:\n",
        "            title_post = title_section.find('a')\n",
        "            post_url = title_post['href']\n",
        "            post_page = get(post_url, headers=headers)\n",
        "            soup_page = BeautifulSoup(post_page.text, 'html.parser')\n",
        "            page_titles = soup_page.find(\"a\", class_=\"title may-blank \")\n",
        "            if page_titles:\n",
        "                page_titles_text = page_titles.text\n",
        "            else:\n",
        "                page_titles_text = \"\"\n",
        "            page_content = soup_page.find_all('div', class_=\"md\")\n",
        "            page_content.pop(0)\n",
        "            content_result = []\n",
        "            comments_result = []\n",
        "            is_content = True\n",
        "            for content in page_content:\n",
        "                content_section = content.find_all('p')\n",
        "                if is_content:\n",
        "                    content_result.append(content_section)\n",
        "                    is_content = False\n",
        "                else:\n",
        "                    comments_result.append(content_section)\n",
        "            results.append({'Title': page_titles_text, 'Content': content_result, 'Comments': comments_result})\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "g8eGyhlv58tK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_urls = ['https://old.reddit.com/r/patientgamers/search?q=early+access&restrict_sr=on&sort=relevance&t=all', \n",
        "              'https://old.reddit.com/r/patientgamers/search?q=early+access&restrict_sr=on&sort=relevance&t=all&count=25&after=t3_val90l',\n",
        "              'https://old.reddit.com/r/patientgamers/search?q=early+access&restrict_sr=on&sort=relevance&t=all&count=50&after=t3_az23ml',\n",
        "              'https://old.reddit.com/r/patientgamers/search?q=early+access&restrict_sr=on&sort=relevance&t=all&count=75&after=t3_fr3d5z',\n",
        "              'https://old.reddit.com/r/patientgamers/search?q=early+access&restrict_sr=on&sort=relevance&t=all&count=100&after=t3_8u9dx6',\n",
        "              'https://old.reddit.com/r/patientgamers/search?q=early+access&restrict_sr=on&sort=relevance&t=all&count=125&after=t3_7hioz9',\n",
        "              'https://old.reddit.com/r/patientgamers/search?q=early+access&restrict_sr=on&sort=relevance&t=all&count=150&after=t3_1i3k22',\n",
        "              'https://old.reddit.com/r/patientgamers/search?q=early+access&restrict_sr=on&sort=relevance&t=all&count=175&after=t3_9poafo',\n",
        "              'https://old.reddit.com/r/patientgamers/search?q=early+access&restrict_sr=on&sort=relevance&t=all&count=200&after=t3_3nho6g',\n",
        "              'https://old.reddit.com/r/patientgamers/search?q=early+access&restrict_sr=on&sort=relevance&t=all&count=225&after=t3_oeafx1']\n",
        "\n",
        "results = search_in_reddit(total_urls)\n",
        "\n",
        "# Save results to a CSV file\n",
        "with open('reddit_posts_final.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    fieldnames = ['Title', 'Content', 'Comments']\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for row in results:\n",
        "        writer.writerow(row)"
      ],
      "metadata": {
        "id": "ezKsk2Rn6AFj"
      },
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}